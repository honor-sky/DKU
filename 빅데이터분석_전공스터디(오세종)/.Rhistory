as.character(sms_corpus[[1]])
sms_corpus_clean<-tm_map(sms_corpus,content_transformer(tolower)) #대->소 변환
sms_corpus_clean<-tm_map(sms_corpus_clean,removeNumbers) #숫자 제거
sms_corpus_clean<-tm_map(sms_corpus_clean,removeWords,stopwords()) #불용어 제거(but,or,and..)
sms_corpus_clean<-tm_map(sms_corpus_clean,removePunctuation) #구두점 제거
sms_corpus_clean<-tm_map(sms_corpus_clean,stemDocument) #형태소 분석
sms_corpus_clean<-tm_map(sms_corpus_clean,stripWhitespace) #추가여백 제거
sms_dtm<-DocumentTermMatrix(sms_corpus_clean)
sms_dtm_train<-sms_dtm[1:4169,]
sms_dtm_test<-sms_dtm[4170:5559,]
sms_train_labels<-sms_raw[1:4169,]$type
sms_test_labels<-sms_raw[4170:5559,]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
#par(mar=c(1,1,1,1))
#wordcloud(sms_corpus_clean,min.freq = 50,random.order = FALSE)
sms_freq_words<-findFreqTerms(sms_dtm_train,5)
sms_dtm_freq_train<-sms_dtm_train[,sms_freq_words]
sms_dtm_freq_test<-sms_dtm_test[,sms_freq_words]
convert_counts<-function(x){
x<-ifelse(x>0,"Yes","No")
}
sms_train<-apply(sms_dtm_freq_train,MARGIN = 2,convert_counts)
sms_test<-apply(sms_dtm_freq_test,MARGIN = 2,convert_counts)
sms_classifier2<-naiveBayes(sms_train,sms_train_labels,laplace = 1)
sms_test_pred2<-predict(sms_classifier2,sms_test)
CrossTable(sms_test_pred2,sms_test_labels,
prop.chisq=FALSE,prop.c=FALSE,prop.r=FALSE,dnn=c('predicted','actual'))
library(FSelector)
library(caret)
library(e1071)  # svm
library(randomForest)
library(class)
library(cvTools)
library(MASS)
library(tidyverse)
vf_oct<-read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv")
#불필요한변수&결측값제거
feature<-vf_oct[,3]
feature<-cbind(feature,vf_oct[,c(6:ncol(vf_oct))])
names(feature)[1]<-"label"
feature
kfold <- function(classifier, ds, cl, fold) {
acc <- c() #fold별acc 저장
for (i in 1:length(fold)) {
ds.train <- ds[-fold[[i]], ]
ds.test <-  ds[fold[[i]], ]
cl.train <- cl[-fold[[i]]]
cl.test <- cl[fold[[i]]]
if (classifier == 'svm') {
model <- svm(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if (classifier == 'rf') {
model <- randomForest(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if(classifier == 'knn') {
pred<-knn(ds.train,ds.test,cl.train,k=3,prob = TRUE) #굳이 여기서 테스트 데이터를 넣어야 하나?
acc[i] <- mean(pred==cl.test)
}
if(classifier=='logi_reg') {
train<-ds.train
train<-cbind(train,cl.train)
names(train)[ncol(train)]<-"label"
#계수 NA 제거
train<-train[,-2]
train<-train[,-5]
train<-train[,-6]
model <- glm(label~.,data=train,family='binomial')
#NA계수 제거
ds.test<-ds.test[,-2]
ds.test<-ds.test[,-5]
ds.test<-ds.test[,-6]
pred <- round(predict(model, ds.test,type='response'),0)
acc[i] <- mean(pred==cl.test)
}
}
return(mean(acc))
}
result_mt<-matrix("",nrow=6,ncol=4) #acc저장할 matrix
rownames(result_mt)<-c(5,6,7,8,9,10)
colnames(result_mt)<-c("svm","rf","knn","logi_reg")
weights <- symmetrical.uncertainty(label~., feature)
set.seed(100)
subset<- cutoff.k(weights, 5)
clean_feature<-subset(feature,complete.cases(feature[,subset]))
clean_feature
ds <- feature[,subset]
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
ds
cl
length(cl)
nrow(ds)
is.na(ds)
weights <- symmetrical.uncertainty(label~., feature)
set.seed(100)
for(i in 5:10){
subset<- cutoff.k(weights, i)
clean_feature<-subset(feature,complete.cases(feature[,subset]))
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
result_mt[i-4,1]<-kfold('svm',ds,cl,fold)
result_mt[i-4,2]<-kfold('rf',ds,cl,fold)
result_mt[i-4,3]<-kfold('knn',ds,cl,fold)
#계수가 NA인 feature 삭제를 고려해 feature를 8개부터 추출
subset<- cutoff.k(weights, i+3)
ds <- feature[,subset]
cl <- factor(feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
result_mt[i-4,4]<-kfold('logi_reg',ds,cl,fold)
}
result_mt
csv <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv",head=TRUE)
csv <- csv[3,-1]
useful_f <- csv[,order(csv)]#p값을 기준으로 정렬된 dataframe
f_name <-colnames(useful_f) #p값을 기준으로 feature명 정렬
new_file <- read.csv("vf_oct_clean_210628.csv")
new_file <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/vf_oct_clean_210628.csv")
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
none_na <- new_file
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
none_na
none_na[complete.cases(none_na[,f_name[1]]),]
complete.cases(none_na[,f_name[1]])
none_na[,f_name[1]]
f_name
csv <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv",head=TRUE)
csv <- csv[3,-1]
useful_f <- csv[,order(csv)]#p값을 기준으로 정렬된 dataframe
f_name <-colnames(useful_f) #p값을 기준으로 feature명 정렬
library(class)
library(cvTools)
acc_k <- c()
k=5
none_na <- new_file
#feature의 개수를 늘리는 반복문
#p-value 기준 하위count개의 feature선정
for (count in 0:20){
#분석하려는 데이터의 결측값들 제거
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
#분석하는 반복문
for(i in 1:k){
tr.idx <- folds$which==1
ds.tr <- none_na[-tr.idx,f_name[1:5+count]]
ds.ts <- none_na[tr.idx,f_name[1:5+count]]
cl.tr <- none_na[-tr.idx,"label"]
cl.ts <- none_na[tr.idx,"label"]
pred <- knn(ds.tr,ds.ts,cl.tr,k=5)
acc_f[i] <- mean(pred==cl.ts) #fold들의 정확도 평균
}
acc_k[count+1] <- mean(acc_f)
}
#분석하려는 데이터의 결측값들 제거
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
tr.idx <- folds$which==1
ds.tr <- none_na[-tr.idx,f_name[1:5]]
ds.ts <- none_na[tr.idx,f_name[1:5]]
cl.tr <- none_na[-tr.idx,"label"]
cl.ts <- none_na[tr.idx,"label"]
ds.tr
csv <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv",head=TRUE)
csv <- csv[3,-1]
useful_f <- csv[,order(csv)]#p값을 기준으로 정렬된 dataframe
f_name <-colnames(useful_f) #p값을 기준으로 feature명 정렬
new_file <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/vf_oct_clean_210628.csv")
acc_k <- c()
k=5
none_na <- new_file
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
tr.idx <- folds$which==1
ds.tr <- none_na[-tr.idx,f_name[1:5]]
ds.ts <- none_na[tr.idx,f_name[1:5]]
ds.tr
ds.ts
csv <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv",head=TRUE)
csv <- csv[3,-1]
useful_f <- csv[,order(csv)]#p값을 기준으로 정렬된 dataframe
f_name <-colnames(useful_f) #p값을 기준으로 feature명 정렬
new_file <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/vf_oct_clean_210628.csv")
#knn분석(fold사용)
library(class)
library(cvTools)
acc_k <- c()
k=5
none_na <- new_file
#feature의 개수를 늘리는 반복문
#p-value 기준 하위count개의 feature선정
for (count in 0:20){
#분석하려는 데이터의 결측값들 제거
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
#분석하는 반복문
for(i in 1:k){
tr.idx <- folds$which==i
ds.tr <- none_na[-tr.idx,f_name[1:5]]
ds.ts <- none_na[tr.idx,f_name[1:5]]
cl.tr <- none_na[-tr.idx,"label"]
cl.ts <- none_na[tr.idx,"label"]
pred <- knn(ds.tr,ds.ts,cl.tr,k=5)
acc_f[i] <- mean(pred==cl.ts) #fold들의 정확도 평균
}
acc_k[count+1] <- mean(acc_f)
}
acc_k #feature의 개수별 정확도
max(acc_k)#11~14개로 할 때 예측정확도 높음
#다중선형 회귀분석
acc <- c()
out.val<- boxplot.stats(none_na$Avg_RNFL_Thickness_value)$out
none_na$Avg_RNFL_Thickness_value[none_na$Avg_RNFL_Thickness_value %in% out.val] <- NA
out.val<- boxplot.stats(none_na$RNFL_I)$out
none_na$RNFL_I[none_na$RNFL_I %in% out.val] <- NA
out.val<- boxplot.stats(none_na$Rim.Area_value)$out
none_na$Rim.Area_value[none_na$Rim.Area_value %in% out.val] <- NA
out.val<- boxplot.stats(none_na$Rim.Area_m_v)$out
none_na$Rim.Area_m_v[none_na$Rim.Area_m_v %in% out.val] <- NA
#feature의 개수를 늘리는 반복문
#p-value 기준 하위count+5개의 feature선정
for (count in 0:30){
none_na <- new_file
predict <- c()
#분석하려는 데이터의 결측값들 제거
for (n in 1:5+count){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
Udata <- none_na[,c(f_name[1:5+count],"label")]
answer <- Udata$label #실제 정답
mod.f <- lm(formula=label~.,data=Udata)
for(l in 1:nrow(Udata)){
pred <- predict(mod.f, Udata[l,f_name[1:5+count]])
predict[l] <- round(pred,0) #예측 결과
}
acc[count+1] <- mean(predict==answer) #feature가 count+5개일 때 정확도
}
acc #feature를 13개로 했을 때 예측 정확도 가장 높음
max(acc)
#로지스틱 회귀분석
acc <- c()
#feature의 개수를 늘리는 반복문
#p-value 기준 하위count+5개의 feature선정
for (count in 0:30){
none_na <- new_file
predict <- c()
#분석하려는 데이터의 결측값들 제거
for (n in 1:5+count){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
Udata <- none_na[,c(f_name[1:5+count],"label")]
answer <- factor(Udata$label) #실제 정답
mod.f <- glm(formula=label~.,data=Udata)
for(l in 1:nrow(Udata)){
pred <- predict(mod.f, Udata[l,f_name[1:5+count]])
predict[l] <- round(pred,0) #예측 결과
}
acc[count+1] <- mean(predict==answer) #feature가 count+5개일 때 정확도
}
acc #feature를 13개로 했을 때 예측 정확도 가장 높음
max(acc)
#svm
library(caret)
library(e1071)  # svm
library(randomForest)
none_na <- new_file
acc_s <- c()
#feature의 개수를 늘리는 반복문
#p-value 기준 하위count개의 feature선정
for (count in 0:20){
#분석하려는 데이터의 결측값들 제거
for (n in 1:5+count){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
#분석하는 반복문
for(i in 1:k){
tr.idx <- folds$which==i
ds.tr <- none_na[-tr.idx,f_name[1:5+count]]
ds.ts <- none_na[tr.idx,f_name[1:5+count]]
cl.tr <- none_na[-tr.idx,"label"]
cl.ts <- none_na[tr.idx,"label"]
model <- svm(ds.tr, cl.tr)
pred <- round(predict(model, ds.ts),0)
acc_f[i] <- mean(pred==cl.ts)
}
acc_s[count+1] <- mean(acc_f)
}
acc_s
max(acc_s)
#RandomForest
none_na <- new_file
acc_r <- c()
for (count in 0:19){
#분석하려는 데이터의 결측값들 제거
for (n in 1:5+count){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
#분석하는 반복문
for(i in 1:k){
tr.idx <- folds$which==i
ds.tr <- none_na[-tr.idx,f_name[1:5+count]]
ds.ts <- none_na[tr.idx,f_name[1:5+count]]
cl.tr <- factor(none_na[-tr.idx,"label"])
cl.ts <- factor(none_na[tr.idx,"label"])
model <- randomForest(ds.tr, cl.tr)
pred <-predict(model, ds.ts)
acc_f[i] <- mean(pred==cl.ts)
}
acc_r[count+1] <- mean(acc_f)
}
acc_r
new_feature<-feature[,subset]$label
new_feature
vf_oct<-read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv")
#불필요한변수&결측값제거
feature<-vf_oct[,3]
feature<-cbind(feature,vf_oct[,c(6:ncol(vf_oct))])
names(feature)[1]<-"label"
kfold <- function(classifier, ds, cl, fold) {
acc <- c() #fold별acc 저장
for (i in 1:length(fold)) {
ds.train <- ds[-fold[[i]], ]
ds.test <-  ds[fold[[i]], ]
cl.train <- cl[-fold[[i]]]
cl.test <- cl[fold[[i]]]
if (classifier == 'svm') {
model <- svm(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if (classifier == 'rf') {
model <- randomForest(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if(classifier == 'knn') {
pred<-knn(ds.train,ds.test,cl.train,k=3,prob = TRUE) #굳이 여기서 테스트 데이터를 넣어야 하나?
acc[i] <- mean(pred==cl.test)
}
if(classifier=='logi_reg') {
train<-ds.train
train<-cbind(train,cl.train)
names(train)[ncol(train)]<-"label"
#계수 NA 제거
train<-train[,-2]
train<-train[,-5]
train<-train[,-6]
model <- glm(label~.,data=train,family='binomial')
#NA계수 제거
ds.test<-ds.test[,-2]
ds.test<-ds.test[,-5]
ds.test<-ds.test[,-6]
pred <- round(predict(model, ds.test,type='response'),0)
acc[i] <- mean(pred==cl.test)
}
}
return(mean(acc))
}
result_mt<-matrix("",nrow=6,ncol=4) #acc저장할 matrix
rownames(result_mt)<-c(5,6,7,8,9,10)
colnames(result_mt)<-c("svm","rf","knn","logi_reg")
weights <- symmetrical.uncertainty(label~., feature)
set.seed(100)
subset<- cutoff.k(weights, 5)
new_feature<-feature[,subset]$label
new_feature
feature
new_feature<-feature$label
new_feature
new_feature<-cbind(new_feature,feature[,subset])
new_feature
names(feature)[1]<-"label"
nrow(new_feature)
clean_feature<-subset(new_feature,complete.cases(new_feature))
nrow(clean_feature)
clean_feature
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
feature<-vf_oct[,3]
feature<-cbind(feature,vf_oct[,c(6:ncol(vf_oct))])
names(feature)[1]<-"label"
kfold <- function(classifier, ds, cl, fold) {
acc <- c() #fold별acc 저장
for (i in 1:length(fold)) {
ds.train <- ds[-fold[[i]], ]
ds.test <-  ds[fold[[i]], ]
cl.train <- cl[-fold[[i]]]
cl.test <- cl[fold[[i]]]
if (classifier == 'svm') {
model <- svm(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if (classifier == 'rf') {
model <- randomForest(ds.train, cl.train)
pred <- predict(model, ds.test)
acc[i] <- mean(pred==cl.test)
}
if(classifier == 'knn') {
pred<-knn(ds.train,ds.test,cl.train,k=3,prob = TRUE) #굳이 여기서 테스트 데이터를 넣어야 하나?
acc[i] <- mean(pred==cl.test)
}
if(classifier=='logi_reg') {
train<-ds.train
train<-cbind(train,cl.train)
names(train)[ncol(train)]<-"label"
#계수 NA 제거
train<-train[,-2]
train<-train[,-5]
train<-train[,-6]
model <- glm(label~.,data=train,family='binomial')
#NA계수 제거
ds.test<-ds.test[,-2]
ds.test<-ds.test[,-5]
ds.test<-ds.test[,-6]
pred <- round(predict(model, ds.test,type='response'),0)
acc[i] <- mean(pred==cl.test)
}
}
return(mean(acc))
}
result_mt<-matrix("",nrow=6,ncol=4) #acc저장할 matrix
rownames(result_mt)<-c(5,6,7,8,9,10)
colnames(result_mt)<-c("svm","rf","knn","logi_reg")
weights <- symmetrical.uncertainty(label~., feature)
set.seed(100)
for(i in 5:10){
subset<- cutoff.k(weights, i)
new_feature<-feature$label
new_feature<-cbind(new_feature,feature[,subset])
names(feature)[1]<-"label"
clean_feature<-subset(new_feature,complete.cases(new_feature))#결측값 제거
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
result_mt[i-4,1]<-kfold('svm',ds,cl,fold)
result_mt[i-4,2]<-kfold('rf',ds,cl,fold)
result_mt[i-4,3]<-kfold('knn',ds,cl,fold)
#계수가 NA인 feature 삭제를 고려해 feature를 8개부터 추출
subset<- cutoff.k(weights, i+3)
new_feature<-feature$label
new_feature<-cbind(new_feature,feature[,subset])
names(feature)[1]<-"label"
clean_feature<-subset(new_feature,complete.cases(new_feature))#결측값 제거
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
result_mt[i-4,4]<-kfold('logi_reg',ds,cl,fold)
}
#불필요한변수&결측값제거
feature<-vf_oct[,3]
feature<-cbind(feature,vf_oct[,c(6:ncol(vf_oct))])
names(feature)[1]<-"label"
result_mt<-matrix("",nrow=6,ncol=4) #acc저장할 matrix
rownames(result_mt)<-c(5,6,7,8,9,10)
colnames(result_mt)<-c("svm","rf","knn","logi_reg")
weights <- symmetrical.uncertainty(label~., feature)
set.seed(100)
i<-5
subset<- cutoff.k(weights, i)
new_feature<-feature$label
new_feature<-cbind(new_feature,feature[,subset])
names(feature)[1]<-"label"
clean_feature<-subset(new_feature,complete.cases(new_feature))#결측값 제거
ds <- clean_feature[,subset]
cl <- factor(clean_feature$label)
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
fold
csv <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/feature_clean.csv",head=TRUE)
csv <- csv[3,-1]
useful_f <- csv[,order(csv)]#p값을 기준으로 정렬된 dataframe
f_name <-colnames(useful_f) #p값을 기준으로 feature명 정렬
new_file <- read.csv("D:/단국대학교/연구실/빅데이터안과진단기술/수행과제/vf_oct_clean_210628.csv")
acc_k <- c()
k=5
none_na <- new_file
none_na
for (n in 1:5){
none_na <- none_na[complete.cases(none_na[,f_name[n]]),]
}
folds <- cvFolds(nrow(none_na),K=k)#폴드 생성
acc_f <- c() #폴드별 예측정확도
tr.idx <- folds$which==1
ds.tr <- none_na[-tr.idx,f_name[1:5]]
ds.tr
f_name
fold <- createFolds(cl, k=5, list=TRUE, returnTrain = FALSE)
